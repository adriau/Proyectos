{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Reducción de dimensionalidad. PCA: Principal Component Analysis\n",
    "\n",
    "Muchos problemas de ML involucrarán muchas features (características) por instancia. No solo todas estas características hacen que el entrenamiento sea extremadamente lento, sino que también pueden hacer que sea mucho más difícil para un método de optimización encontrar una buena solución.\n",
    "\n",
    "Este problema a menudo se conoce como la **maldición de la dimensionalidad**. En un problema del mundo real, a menudo es posible reducir considerablemente el número de funciones. Esto da como resultado convertir un problema intratable en uno tratable.\n",
    "\n",
    "El objetivo es eliminar la cantidad máxima de funciones y minimizar la pérdida de información relacionada con una tarea específica. Sin embargo, la reducción de la dimensionalidad siempre causará alguna pérdida de información. También hará que nuestra canalización sea un poco más compleja y, por lo tanto, más difícil de mantener.\n",
    "\n",
    "La reducción de la dimensionalidad generalmente se lleva a cabo para **acelerar el entrenamiento** y es extremadamente útil para la visualización de datos al proyectar los datos en un espacio de 2-3 dimensional.\n",
    "\n",
    "La visualización de datos también es importante para comunicar nuestros hallazgos a personas que no son datascientists. En particular, a las personas que tomarán decisiones y que utilizarán los resultados.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div style=\"text-align:center;\">\n",
    "    <img style=\"width:50%\" src=\"img/img_1.png\" />\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img style=\"width:50%\" src=\"img/img.png\" />\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Principales aproximaciones a la reducción de dimensianalidad"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Proyección\n",
    "\n",
    "En la mayoría de los problemas del mundo real, las instancias de entrenamiento no se distribuyen uniformemente entre las dimensiones. Muchas características son casi constantes, mientras que otras están altamente correlacionadas. Como resultado, todas las instancias de entrenamiento se asientan en un **subespacio** dimensional mucho más bajo del espacio dimensional alto.\n",
    "\n",
    "Aquí tenemos un ejemplo:\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img style=\"width:50%\" src=\"img/subspace_projection.png\" width = \"800\" height = \"500\"/>\n",
    "</div>\n",
    "\n",
    "Si proyectamos perpendicularmente cada instancia de entrenamiento en el subespacio, obtenemos un nuevo conjunto de datos 2D representado de la siguiente manera:\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img style=\"width:50%\" src=\"img/2d_projection.png\" width = \"800\" height = \"500\"/>\n",
    "</div>\n",
    "\n",
    "Sin embargo, la proyección no siempre es el mejor enfoque para la reducción de la dimensionalidad. En muchos casos, el subespacio puede girar y girar.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Aprendizaje múltiple\n",
    "\n",
    "Un colector 2D es una forma 2D que se puede doblar y torcer en un espacio de dimensiones superior. De manera más general, una variedad d-dimensional es parte de un espacio n-dimensional (donde $d < n$), que localmente se parece a un hiperlugar d-dimensional.\n",
    "\n",
    "Muchos algoritmos de reducción de dimensionalidad funcionan modelando la variedad en la que se encuentran las instancias de entrenamiento, esto se denomina **Aprendizaje múltiple**. Se basa en la *suposición múltiple*, también llamada *hipótesis múltiple*, que establece que la mayoría de los conjuntos de datos de alta dimensión del mundo real se encuentran cerca de una variedad de dimensiones mucho más bajas. Esta suposición se observa muy a menudo empíricamente.\n",
    "\n",
    "Si tuviéramos que generar todas las imágenes aleatorias posibles en una cuadrícula de '28x28', solo una pequeña fracción de ellas se vería como dígitos escritos a mano. En otras palabras, los grados de libertad disponibles para nosotros si tuviéramos que crear una imagen digital son muy bajos en comparación con el grado de libertad que tenemos cuando queremos crear cualquier imagen que queramos. Estas restricciones tienden a comprimir el conjunto de datos en una variedad de menor dimensión.\n",
    "\n",
    "Una suposición adicional implícita es que la tarea en cuestión (ya sea la regresión o la clasificación) sería mucho más fácil si se realizara en el espacio múltiple de dimensiones inferiores. Sin embargo, esta suposición no siempre se cumple.\n",
    "\n",
    "Ejemplos a seguir:\n",
    "\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img style=\"width:50%\" src=\"img/manifold_classification.png\" width = \"800\" height = \"500\"/>\n",
    "</div>\n",
    "\n",
    "En definitiva, reducir la dimensionalidad de nuestro conjunto de datos acelerará el entrenamiento, pero no garantiza una solución más sencilla. Todo depende del conjunto de datos y de la tarea en cuestión.\n",
    "\n",
    "Ahora veremos el algoritmo de reducción de dimensionalidad más popular, el PCA."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PCA: Principal Component Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "El análisis de componentes principales es, con mucho, el algoritmo de reducción de dimensionalidad más popular.\n",
    "\n",
    "PCA es un algoritmo sensible a la escala relativa de las variables originales. El método de PCA permite por lo tanto “condensar” la información aportada por múltiples variables en solo unas pocas componentes. Esto lo convierte en un método muy útil de aplicar previa utilización de otras técnicas estadísticas tales como regresión, clustering… Aun así no hay que olvidar que sigue siendo necesario disponer del valor de las variables originales para calcular las componentes.\n",
    "\n",
    "\n",
    "Para aplicar PCA, primero, identificaremos el hiperplano que se encuentra más cerca de los datos. Luego, proyectaremos los datos en él."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preservación de la varianza\n",
    "\n",
    "Queremos elegir un hiperplano que conserve la mayor varianza dentro de los datos. A continuación vamos a ver 3 intentos de proyecciones para 3 hiperplanos elegidos (eje 1D):\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img style=\"width:66%\" src=\"img/2D_variance_projection.png\" width = \"800\" height = \"500\"/>\n",
    "</div>\n",
    "\n",
    "**Parece razonable seleccionar el eje que conserva la máxima cantidad de varianza ya que lo más probable es que pierda menos información que otras proyecciones.**\n",
    "\n",
    "Otra forma alternativa de verlo es que al elegir el eje de la línea sólida (en lugar de los ejes con líneas discontinuas), estamos minimizando la distancia cuadrática media entre los puntos originales y sus proyecciones en el eje elegido. Esta es la idea simple detrás de PCA."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Componentes Principales\n",
    "\n",
    "PCA identifica el eje que representa la mayor cantidad de variación en el conjunto de entrenamiento. También encuentra un segundo eje, ortogonal al primero, que representa la mayor cantidad de varianza restante.\n",
    "\n",
    "Si estamos considerando un conjunto de datos de mayor dimensión, PCA también encontraría un tercer eje, un cuarto, un quinto, y así sucesivamente... (tantos ejes como el número de dimensiones en el conjunto de datos).\n",
    "\n",
    "El `i`ésimo eje se denomina `i`ésimo **componente principal** de los datos.\n",
    "\n",
    "Entonces, ¿cómo podemos encontrar los componentes principales de un conjunto de entrenamiento? Existe una técnica estándar de vectorización de matrices llamada *Descomposición de valores singulares (SVD)*. Esta técnica descompone el conjunto de entrenamiento $X$ en $X=U \\Sigma V^T$, $V$ que contiene los vectores unitarios que definen todos los componentes principales que buscamos:\n",
    "\n",
    "\n",
    "$$V=\n",
    "  \\begin{pmatrix}\n",
    "    \\vert & \\vert & \\dots & \\vert \\\\\n",
    "    c_1 & c_2 & \\dots & c_n \\\\\n",
    "    \\vert & \\vert & \\dots & \\vert \\\\\n",
    "  \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Ahora vamos a extraer los los componentes principales de un dataset usando la implementación `svd` de numpy."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Demo PCA\n",
    "\n",
    "### Generación de datos de muestra\n",
    "\n",
    "Vamos a generar algunos datos aleatorios:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "data": {
      "text/plain": "((100,), (100,))"
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.linspace(start=-1., stop=1., num=100) #generación de un array np de 100 puntos equipespaciados entre un mínimo y un máximo\n",
    "y = X + np.random.normal(size=100)/7. #el eje y será igual al X añadiendo cierta aleatoriedad (la función normal genera dichos números aleatorios con una distribución normal)\n",
    "X.shape, y.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X, y, c='blue')\n",
    "plt.xlabel('$X_1$')\n",
    "plt.ylabel('$X_2$')\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "data": {
      "text/plain": "(100, 2)"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.concatenate((X[..., None], y[..., None]), axis=1) #creamos un array bidimensional de 100x2\n",
    "X.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-1.        , -0.86299133],\n       [-0.97979798, -0.8084679 ],\n       [-0.95959596, -0.93023824],\n       [-0.93939394, -1.05728927],\n       [-0.91919192, -0.84622674],\n       [-0.8989899 , -0.87636642],\n       [-0.87878788, -0.98813344],\n       [-0.85858586, -0.93642831],\n       [-0.83838384, -1.16753479],\n       [-0.81818182, -0.82609607],\n       [-0.7979798 , -0.69243962],\n       [-0.77777778, -0.58687043],\n       [-0.75757576, -0.80653551],\n       [-0.73737374, -0.86364044],\n       [-0.71717172, -0.62395435],\n       [-0.6969697 , -0.74504323],\n       [-0.67676768, -0.62919243],\n       [-0.65656566, -0.80030577],\n       [-0.63636364, -0.41269058],\n       [-0.61616162, -0.63397425],\n       [-0.5959596 , -0.75185606],\n       [-0.57575758, -0.67796856],\n       [-0.55555556, -0.60668709],\n       [-0.53535354, -0.5099248 ],\n       [-0.51515152, -0.53822523],\n       [-0.49494949, -0.52740995],\n       [-0.47474747, -0.61639267],\n       [-0.45454545, -0.31538427],\n       [-0.43434343, -0.58846803],\n       [-0.41414141, -0.287172  ],\n       [-0.39393939, -0.3830161 ],\n       [-0.37373737, -0.60176767],\n       [-0.35353535, -0.4028458 ],\n       [-0.33333333, -0.40605514],\n       [-0.31313131, -0.25403666],\n       [-0.29292929, -0.46022848],\n       [-0.27272727, -0.06474811],\n       [-0.25252525, -0.14840737],\n       [-0.23232323, -0.3101884 ],\n       [-0.21212121, -0.1035743 ],\n       [-0.19191919, -0.10094339],\n       [-0.17171717, -0.4771227 ],\n       [-0.15151515, -0.26654149],\n       [-0.13131313, -0.44055478],\n       [-0.11111111, -0.25274125],\n       [-0.09090909,  0.03611199],\n       [-0.07070707, -0.16343329],\n       [-0.05050505,  0.12074388],\n       [-0.03030303,  0.09345598],\n       [-0.01010101,  0.06486226],\n       [ 0.01010101, -0.14202501],\n       [ 0.03030303,  0.11534723],\n       [ 0.05050505, -0.10580928],\n       [ 0.07070707,  0.18908295],\n       [ 0.09090909,  0.0024551 ],\n       [ 0.11111111, -0.1417695 ],\n       [ 0.13131313,  0.33977978],\n       [ 0.15151515,  0.0172224 ],\n       [ 0.17171717,  0.03694793],\n       [ 0.19191919,  0.34806084],\n       [ 0.21212121,  0.24554229],\n       [ 0.23232323,  0.3415226 ],\n       [ 0.25252525, -0.01932475],\n       [ 0.27272727,  0.23530505],\n       [ 0.29292929,  0.34207406],\n       [ 0.31313131,  0.23190084],\n       [ 0.33333333,  0.5502506 ],\n       [ 0.35353535,  0.37554172],\n       [ 0.37373737,  0.43124928],\n       [ 0.39393939,  0.577564  ],\n       [ 0.41414141,  0.387351  ],\n       [ 0.43434343,  0.24721054],\n       [ 0.45454545,  0.47215487],\n       [ 0.47474747,  0.65814038],\n       [ 0.49494949,  0.57969015],\n       [ 0.51515152,  0.61521965],\n       [ 0.53535354,  0.42354007],\n       [ 0.55555556,  0.35700514],\n       [ 0.57575758,  0.39468232],\n       [ 0.5959596 ,  0.65030379],\n       [ 0.61616162,  0.77290238],\n       [ 0.63636364,  0.67676389],\n       [ 0.65656566,  0.620827  ],\n       [ 0.67676768,  0.63718638],\n       [ 0.6969697 ,  1.08717026],\n       [ 0.71717172,  0.39704484],\n       [ 0.73737374,  0.81503858],\n       [ 0.75757576,  0.88820349],\n       [ 0.77777778,  0.73892126],\n       [ 0.7979798 ,  0.86342927],\n       [ 0.81818182,  0.72311052],\n       [ 0.83838384,  1.04765619],\n       [ 0.85858586,  0.91283348],\n       [ 0.87878788,  0.81452937],\n       [ 0.8989899 ,  0.85738849],\n       [ 0.91919192,  0.94871122],\n       [ 0.93939394,  1.21826607],\n       [ 0.95959596,  1.17713358],\n       [ 0.97979798,  0.97923045],\n       [ 1.        ,  1.10237583]])"
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_centered = X - X.mean(axis=0) #centramos el array bidimensional respecto la media para que tenga una media igual a 0 y desviación típica 1\n",
    "#Si nos fijamos la columna 0(X) no se ve alterada porque los puntos ya eran equiespaciados, pero sí que se ven alterados los puntos de la columna 1(y)\n",
    "X_centered"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3.2. Identificación de Componentes Principales: SVD (Descomposición en valores singulares)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "U, s, Vt = np.linalg.svd(a=X_centered) #svd: singular value descomposition https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "c1 = Vt.T[:, 0]\n",
    "c2 = Vt.T[:, 1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([0.6845505 , 0.72896544]), array([ 0.72896544, -0.6845505 ]))"
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1, c2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Los dos puntos c1 y c2 en 2D representan los puntos del vector unitario (origen en (0,0)) correspondientes a los 2 componentes principales (ejes que conservan la varianza). PCA asume que el conjunto de datos se centra en el origen, pero la implementación de `scikit-learn` se encarga de centrar los datos."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3.3. Projección en $d$ dimensiones\n",
    "\n",
    "Una vez que hemos identificado todos los componentes principales, podemos reducir la dimensionalidad del dataset a $d$ dimensiones proyectándolo en el hiperplano definido por los primeros $d$ componentes principales.\n",
    "\n",
    "La selección de este hiperplano garantiza que la proyección conserve la mayor varianza posible.\n",
    "\n",
    "Para proyectar el conjunto de entrenamiento en el hiperplano y obtener un conjunto de datos reducido $X_{d-proj}$ de dimensionalidad $d$, calculamos la multiplicación matricial de la matriz del conjunto de entrenamiento $X$ por la matriz $W_{d}$.\n",
    "$W_{d}$ es la matriz que contiene las primeras $d$ columnas de $V$ que representan los componentes principales.\n",
    "\n",
    "$$X_{d-proj}=XW_d$$\n",
    "\n",
    "Vamos a hacerlo en Python:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "W2 = Vt.T[:, :2]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "data": {
      "text/plain": "(100, 2)"
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2D = X_centered.dot(W2)\n",
    "X2D.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Utlizando Scikit-Learn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2) # Si pongo None los coge e intenta hacerlo para todos.\n",
    "# Crea el numero de componentes que le indicamos."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "X2D = pca.fit_transform(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "assert np.all(np.abs(pca.components_) == np.abs(W2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3.4. Ratio de varianza definida/explicada (Explained Variance Ratio)\n",
    "\n",
    "Ahora vamos a mostrarel porcentaje de varianza explicada/definida por cada una de las variables/componentes de la muestra. (El porcentaje o ratio indica la proporción de la varianza del conjunto de datos que se encuentra a lo largo de cada componente principal)\n",
    "\n",
    "Si consultadmos nuestro modelo de scikit-learn nos devuelve:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([0.98684695, 0.01315305]), 1.0)"
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_, np.sum(pca.explained_variance_ratio_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Este valor nos quiere decir que utilizando las 2 primeras variables (sobretodo la primera que ya aporta un 98%) obtendríamos un 99% de la información que nos aporta el dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  3.3.5. Selección del número correcto de dimensiones\n",
    "\n",
    "Deberíamos elegir la número de dimensiones que sumen una gran parte de la varianza que se encuentra dentro del conjunto de datos original (Ejemplo: $>=95\\%$), a menos que estemos reduciendo la dimensionalidad para visualizar los datos.\n",
    "\n",
    "Hagámoslo en scikit-learn:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pca = PCA()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pca.fit(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cumsum = np.cumsum(pca.explained_variance_ratio_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "d = np.argmax(cumsum >= .95) + 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "d"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Luego volvemos a ejecutar el entrenamiento PCA usando $d$. Sin embargo, hay una opción mucho mejor, la de especificar un valor flotante para `n_components` como la relación de varianza que queremos conservar."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [],
   "source": [
    "X_reduced = pca.fit_transform(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## EXTRA: PCA para compresión\n",
    "\n",
    "Después de la reducción de dimensionalidad, el conjunto de entrenamiento ocupa mucho menos espacio.\n",
    "\n",
    "También es posible descomprimir el conjunto de datos reducido a $784$ (en el caso de MNIST) aplicando **la transformación inversa de la proyección PCA**. Esto no nos devolverá los datos originales ya que el 5 % de la varianza se perdió durante la compresión, pero probablemente estará cerca del conjunto de datos original.\n",
    "\n",
    "La distancia cuadrática media entre el conjunto de datos original y el conjunto de datos descomprimido se denomina error de reconstrucción.\n",
    "\n",
    "Si lo queremos calcular con scikit-learn:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "X, y = datasets.fetch_openml(name='mnist_784', return_X_y=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "data": {
      "text/plain": "((46900, 784), (23100, 784), (46900,), (23100,))"
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "pca = PCA(n_components=154)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [],
   "source": [
    "X_reduced = pca.fit_transform(X_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "X_recovered = pca.inverse_transform(X_reduced)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001B[0m in \u001B[0;36mget_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n\u001B[1;32m   3360\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3361\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_engine\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_loc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcasted_key\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3362\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mKeyError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0merr\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/_libs/index.pyx\u001B[0m in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/_libs/index.pyx\u001B[0m in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32mpandas/_libs/hashtable_class_helper.pxi\u001B[0m in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32mpandas/_libs/hashtable_class_helper.pxi\u001B[0m in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_8800/3249458077.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mimshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_train\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m28\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m28\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcmap\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'binary'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0maxis\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'off'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001B[0m in \u001B[0;36m__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3456\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnlevels\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3457\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_getitem_multilevel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3458\u001B[0;31m             \u001B[0mindexer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_loc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3459\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mis_integer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mindexer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3460\u001B[0m                 \u001B[0mindexer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mindexer\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001B[0m in \u001B[0;36mget_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n\u001B[1;32m   3361\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_engine\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_loc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcasted_key\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3362\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mKeyError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0merr\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3363\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0merr\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3364\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3365\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mis_scalar\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0misna\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhasnans\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 0"
     ]
    }
   ],
   "source": [
    "plt.imshow(X_train[0].reshape(28, 28), cmap='binary')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_recovered[0].reshape(28, 28), cmap='binary')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "La siguiente es la ecuación de la transformación inversa:\n",
    "\n",
    "$$X_{recovered}=X_{d-proj}W_{d}^{T}$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## EXTRA 2: PCA aleatorio\n",
    "\n",
    "Podemos usar un algoritmo estocástico llamado *PCA aleatorio* que encuentra rápidamente una aproximación de los primeros $d$ componentes principales. Su complejidad computacional es $O(m \\times d^{2})+O(d^3)$ en lugar de $O(m \\times n^{2})+O(n^3)$ de SVD. Entonces es dramáticamente más rápido que SVD cuando $d << n$.\n",
    "\n",
    "Usémoslo con `scikit-learn`:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rnd_pca = PCA(n_components=154, svd_solver='randomized')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_reduced = rnd_pca.fit_transform(X_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## EXTRA 3: PCA incremental\n",
    "\n",
    "Un problema con las implementaciones anteriores es que requieren que todos los datos de entrenamiento quepan en la memoria. Afortunadamente, se han desarrollado algoritmos de **PCA incremental** que permiten dividir el conjunto de entrenamiento en mini lotes y alimentarlos uno a uno a la vez al algoritmo IPCA. Esto es útil cuando se tienen conjuntos de entrenamiento grandes o se está ejecutando online.\n",
    "\n",
    "Experimentemos con PCA incremental usando scikit-learn:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_batches = 100\n",
    "inc_pca = IncrementalPCA(n_components=154)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for X_batch in np.array_split(X_train, n_batches):\n",
    "    inc_pca.partial_fit(X_batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_reduced = inc_pca.transform(X_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}