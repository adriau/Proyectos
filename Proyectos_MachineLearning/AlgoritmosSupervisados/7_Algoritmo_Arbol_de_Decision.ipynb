{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5ANp2r_H1p-"
   },
   "source": [
    "# Árboles de decisión\n",
    "\n",
    "**Algoritmo de clasificación y regresión**\n",
    "\n",
    "Es un **modelo predictivo** que divide el espacio de los predictores agrupando observaciones con valores similares para la variable respuesta o dependiente. Los árboles de decisión se emplean en diversos ámbitos para analizar situaciones complejas con múltiples posibilidades de decisión y escoger la mejor.\n",
    "\n",
    "Son representaciones gráficas de posibles soluciones a una decisión basadas en ciertas condiciones. Es uno de los algoritmos de aprendizaje supervisado más utilizados en machine learning y pueden realizar tareas de clasificación o regresión.\n",
    "\n",
    "Principales características de los árboles de decision:\n",
    "* Clasifica mediane el valor de los atributos.\n",
    "* Es una técnica de clasificación muy extendida.\n",
    "* Es fácilmente entendible por personas no expertas.\n",
    "* También pueden utilizarse en técnicas de regresión como herramienta de predicción numérica para estimar valores reales (por ejemplo: ventas al mes, coste de compra de un vehículo...).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Importación de paquetes y dataset\n",
    "\n",
    "Para entender los árboles de decisión, comencemos por construir uno y consultar sus predicciones:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GRmElfIEH1qD"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EX8X2GNeH1qF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'data': array([[5.1, 3.5, 1.4, 0.2],\n        [4.9, 3. , 1.4, 0.2],\n        [4.7, 3.2, 1.3, 0.2],\n        [4.6, 3.1, 1.5, 0.2],\n        [5. , 3.6, 1.4, 0.2],\n        [5.4, 3.9, 1.7, 0.4],\n        [4.6, 3.4, 1.4, 0.3],\n        [5. , 3.4, 1.5, 0.2],\n        [4.4, 2.9, 1.4, 0.2],\n        [4.9, 3.1, 1.5, 0.1],\n        [5.4, 3.7, 1.5, 0.2],\n        [4.8, 3.4, 1.6, 0.2],\n        [4.8, 3. , 1.4, 0.1],\n        [4.3, 3. , 1.1, 0.1],\n        [5.8, 4. , 1.2, 0.2],\n        [5.7, 4.4, 1.5, 0.4],\n        [5.4, 3.9, 1.3, 0.4],\n        [5.1, 3.5, 1.4, 0.3],\n        [5.7, 3.8, 1.7, 0.3],\n        [5.1, 3.8, 1.5, 0.3],\n        [5.4, 3.4, 1.7, 0.2],\n        [5.1, 3.7, 1.5, 0.4],\n        [4.6, 3.6, 1. , 0.2],\n        [5.1, 3.3, 1.7, 0.5],\n        [4.8, 3.4, 1.9, 0.2],\n        [5. , 3. , 1.6, 0.2],\n        [5. , 3.4, 1.6, 0.4],\n        [5.2, 3.5, 1.5, 0.2],\n        [5.2, 3.4, 1.4, 0.2],\n        [4.7, 3.2, 1.6, 0.2],\n        [4.8, 3.1, 1.6, 0.2],\n        [5.4, 3.4, 1.5, 0.4],\n        [5.2, 4.1, 1.5, 0.1],\n        [5.5, 4.2, 1.4, 0.2],\n        [4.9, 3.1, 1.5, 0.2],\n        [5. , 3.2, 1.2, 0.2],\n        [5.5, 3.5, 1.3, 0.2],\n        [4.9, 3.6, 1.4, 0.1],\n        [4.4, 3. , 1.3, 0.2],\n        [5.1, 3.4, 1.5, 0.2],\n        [5. , 3.5, 1.3, 0.3],\n        [4.5, 2.3, 1.3, 0.3],\n        [4.4, 3.2, 1.3, 0.2],\n        [5. , 3.5, 1.6, 0.6],\n        [5.1, 3.8, 1.9, 0.4],\n        [4.8, 3. , 1.4, 0.3],\n        [5.1, 3.8, 1.6, 0.2],\n        [4.6, 3.2, 1.4, 0.2],\n        [5.3, 3.7, 1.5, 0.2],\n        [5. , 3.3, 1.4, 0.2],\n        [7. , 3.2, 4.7, 1.4],\n        [6.4, 3.2, 4.5, 1.5],\n        [6.9, 3.1, 4.9, 1.5],\n        [5.5, 2.3, 4. , 1.3],\n        [6.5, 2.8, 4.6, 1.5],\n        [5.7, 2.8, 4.5, 1.3],\n        [6.3, 3.3, 4.7, 1.6],\n        [4.9, 2.4, 3.3, 1. ],\n        [6.6, 2.9, 4.6, 1.3],\n        [5.2, 2.7, 3.9, 1.4],\n        [5. , 2. , 3.5, 1. ],\n        [5.9, 3. , 4.2, 1.5],\n        [6. , 2.2, 4. , 1. ],\n        [6.1, 2.9, 4.7, 1.4],\n        [5.6, 2.9, 3.6, 1.3],\n        [6.7, 3.1, 4.4, 1.4],\n        [5.6, 3. , 4.5, 1.5],\n        [5.8, 2.7, 4.1, 1. ],\n        [6.2, 2.2, 4.5, 1.5],\n        [5.6, 2.5, 3.9, 1.1],\n        [5.9, 3.2, 4.8, 1.8],\n        [6.1, 2.8, 4. , 1.3],\n        [6.3, 2.5, 4.9, 1.5],\n        [6.1, 2.8, 4.7, 1.2],\n        [6.4, 2.9, 4.3, 1.3],\n        [6.6, 3. , 4.4, 1.4],\n        [6.8, 2.8, 4.8, 1.4],\n        [6.7, 3. , 5. , 1.7],\n        [6. , 2.9, 4.5, 1.5],\n        [5.7, 2.6, 3.5, 1. ],\n        [5.5, 2.4, 3.8, 1.1],\n        [5.5, 2.4, 3.7, 1. ],\n        [5.8, 2.7, 3.9, 1.2],\n        [6. , 2.7, 5.1, 1.6],\n        [5.4, 3. , 4.5, 1.5],\n        [6. , 3.4, 4.5, 1.6],\n        [6.7, 3.1, 4.7, 1.5],\n        [6.3, 2.3, 4.4, 1.3],\n        [5.6, 3. , 4.1, 1.3],\n        [5.5, 2.5, 4. , 1.3],\n        [5.5, 2.6, 4.4, 1.2],\n        [6.1, 3. , 4.6, 1.4],\n        [5.8, 2.6, 4. , 1.2],\n        [5. , 2.3, 3.3, 1. ],\n        [5.6, 2.7, 4.2, 1.3],\n        [5.7, 3. , 4.2, 1.2],\n        [5.7, 2.9, 4.2, 1.3],\n        [6.2, 2.9, 4.3, 1.3],\n        [5.1, 2.5, 3. , 1.1],\n        [5.7, 2.8, 4.1, 1.3],\n        [6.3, 3.3, 6. , 2.5],\n        [5.8, 2.7, 5.1, 1.9],\n        [7.1, 3. , 5.9, 2.1],\n        [6.3, 2.9, 5.6, 1.8],\n        [6.5, 3. , 5.8, 2.2],\n        [7.6, 3. , 6.6, 2.1],\n        [4.9, 2.5, 4.5, 1.7],\n        [7.3, 2.9, 6.3, 1.8],\n        [6.7, 2.5, 5.8, 1.8],\n        [7.2, 3.6, 6.1, 2.5],\n        [6.5, 3.2, 5.1, 2. ],\n        [6.4, 2.7, 5.3, 1.9],\n        [6.8, 3. , 5.5, 2.1],\n        [5.7, 2.5, 5. , 2. ],\n        [5.8, 2.8, 5.1, 2.4],\n        [6.4, 3.2, 5.3, 2.3],\n        [6.5, 3. , 5.5, 1.8],\n        [7.7, 3.8, 6.7, 2.2],\n        [7.7, 2.6, 6.9, 2.3],\n        [6. , 2.2, 5. , 1.5],\n        [6.9, 3.2, 5.7, 2.3],\n        [5.6, 2.8, 4.9, 2. ],\n        [7.7, 2.8, 6.7, 2. ],\n        [6.3, 2.7, 4.9, 1.8],\n        [6.7, 3.3, 5.7, 2.1],\n        [7.2, 3.2, 6. , 1.8],\n        [6.2, 2.8, 4.8, 1.8],\n        [6.1, 3. , 4.9, 1.8],\n        [6.4, 2.8, 5.6, 2.1],\n        [7.2, 3. , 5.8, 1.6],\n        [7.4, 2.8, 6.1, 1.9],\n        [7.9, 3.8, 6.4, 2. ],\n        [6.4, 2.8, 5.6, 2.2],\n        [6.3, 2.8, 5.1, 1.5],\n        [6.1, 2.6, 5.6, 1.4],\n        [7.7, 3. , 6.1, 2.3],\n        [6.3, 3.4, 5.6, 2.4],\n        [6.4, 3.1, 5.5, 1.8],\n        [6. , 3. , 4.8, 1.8],\n        [6.9, 3.1, 5.4, 2.1],\n        [6.7, 3.1, 5.6, 2.4],\n        [6.9, 3.1, 5.1, 2.3],\n        [5.8, 2.7, 5.1, 1.9],\n        [6.8, 3.2, 5.9, 2.3],\n        [6.7, 3.3, 5.7, 2.5],\n        [6.7, 3. , 5.2, 2.3],\n        [6.3, 2.5, 5. , 1.9],\n        [6.5, 3. , 5.2, 2. ],\n        [6.2, 3.4, 5.4, 2.3],\n        [5.9, 3. , 5.1, 1.8]]),\n 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n 'frame': None,\n 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...',\n 'feature_names': ['sepal length (cm)',\n  'sepal width (cm)',\n  'petal length (cm)',\n  'petal width (cm)'],\n 'filename': 'C:\\\\Users\\\\AdrianAlvarezCastro\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\datasets\\\\data\\\\iris.csv'}"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "iris"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. División del dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "Lel1EicRH1qG"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "QQBhwwuNH1qG",
    "outputId": "022f70f0-f1d7-4b57-8548-d05d5f1c9d7f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "((150, 2), (150,))"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = iris.data[:, 2:]  # Petal length and width\n",
    "y = iris.target\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Creación del modelo de Árboles de decisión"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "8UkZGcu4H1qH"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "JVG44ut1H1qI"
   },
   "outputs": [],
   "source": [
    "tree_clf = DecisionTreeClassifier(max_depth=2) #max_depth indica la profundidad maxima del arbol.\n",
    "                                               # si no se pone nada el algoritmo selecciona de manera automatica"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Entrenamiento"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "gxA9xixJH1qJ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1wrI3S6-H1qK",
    "outputId": "74e8590e-b7aa-492f-9255-8699abf1cec6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "DecisionTreeClassifier(max_depth=2)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Visualización del árbol de decisión\n",
    "\n",
    "Podemos visualizar el árbol de decisiones utilizando el método export_graphiz() para exportar un archivo de representación gráfica y luego transformarlo a png:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "y2qkCy-BH1qK"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "0fKA4-EMH1qL"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5NEYNX2zH1qM"
   },
   "outputs": [],
   "source": [
    "export_graphviz(tree_clf, \n",
    "                out_file='./img/iris_tree.dot',\n",
    "                feature_names=iris.feature_names[2:], # Nombre de las caracteristicas\n",
    "                class_names=iris.target_names,  # Nombre de las clases\n",
    "                rounded=True,\n",
    "                filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ox00VUj-H1qM"
   },
   "source": [
    "Convertimos el archivo gráfico en un archivo .png:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "JojiidkqH1qN",
    "outputId": "0eed8cc1-b684-4951-889c-36709fe3ae69"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: gvwrite_no_z problem 8192\n"
     ]
    }
   ],
   "source": [
    "! dot -Tpng ./img/iris_tree.dot -o ./img/iris_tree.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hc9QZBS8H1qN"
   },
   "source": [
    "Y este es el resultado:\n",
    "\n",
    "![Resultado](img/iris_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9tBJcb3H1qN"
   },
   "source": [
    "## 8. Realización de predicciones\n",
    "\n",
    "Para clasificar un nuevo punto de datos, comenzamos en el nodo raíz del gráfico (en la parte superior) y respondemos las preguntas binarias hasta que llegamos a una hoja final.\n",
    "Esa última hoja representa la clase a la cuál se corresponde.\n",
    "Una de las cualidades de los árboles de decisión es que requieren muy poca preparación de datos. De hecho, no requieren escalado o normalizado de las características.\n",
    "Un nodo de árbol tiene los siguientes atributos:\n",
    "\n",
    "* samples (muestras): cuenta cuántas instancias de entrenamiento están sentadas en el nodo.\n",
    "* value (valor): nos dice cuántas instancias de cada clase están configuradas en el nodo.\n",
    "* gini: mide la impureza de los nodos (nodo puro == 0)\n",
    "\n",
    "La siguiente ecuación muestra cómo el algoritmo de entrenamiento calcula el índice o puntuación Gini del i-ésimo nodo:\n",
    "\n",
    "$$G_i=1-\\sum_{k=1}^n{p_{i,k}}^2$$\n",
    "\n",
    "Donde $p_{i,k}$ es la proporción o ratio de instancias de clase $k$ entre las instancias de entrenamiento en ese nodo en particular. En nuestro caso: $k \\in \\{1,2,3\\}$.\n",
    "\n",
    "`Scikit-learn` usa el algoritmo CART, que produce solo árboles binarios. Los nodos que no son hojas solo tienen dos hijos. Sin embargo, otros algoritmos como ID3 pueden producir árboles de decisión con nodos que tienen más de 2 hijos.\n",
    "\n",
    "La siguiente figura muestra los límites de decisión de nuestro árbol de decisión (los árboles de decisión tienden a crear líneas/rectángulos/cuadros/.. y dividen el espacio de características de forma lineal pero iterativamente):\n",
    " \n",
    "![Boundaries](img/decision_tree_boundaries.png)\n",
    "\n",
    "En general, los árboles de decisión son intuitivos y sus predicciones son fácilmente interpretables. Estos tipos de modelos se denominan modelos de **caja blanca**. Por contra, los RandomForest  y las redes neuronales generalmente se consideran modelos de caja negra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uc-L7UPaH1qO"
   },
   "source": [
    "### 8.1. Estimando las probabilidades de pertenencia a cada clase\n",
    "\n",
    "Un árbol de decisión también puede estimar la probabilidad de que cierta instancia pertenezca a cierta clase. Simplemente devuelve el ratio o proporción de esa clase sobre la suma de todas las instancias en la hoja.\n",
    "\n",
    "Podemos comprobarlo con el método predict_proba de scikit-learn:\n",
    "\n",
    "En este ejemplo, si indicamos que la longitud del pétalo es 5 y el ancho es 1.5, la probabilidad de ser de clase 0 será 0, de clase 1 0.9 y de clase 2 0.09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Lk7vr2DOH1qP",
    "outputId": "c6f682db-cada-47b5-d433-d1933566bb6e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.        , 0.90740741, 0.09259259]])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict_proba([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "pwd8tLEWH1qP",
    "outputId": "3149918c-42fa-436d-d71d-7f49212293e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([1])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKze1MLoH1qP"
   },
   "source": [
    "\n",
    "Nota: obtendremos la misma probabilidad siempre que estemos en un mismo cuadro asignado a la hoja. No importa si nuestro nuevo punto de datos se acerca a los márgenes de decisión (decision boundaries)."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Índice GINI\n",
    "\n",
    "El índice Gini es el nombre de la función de coste utilizada para evaluar las divisiones en el conjunto de datos.\n",
    "\n",
    "Una división en el conjunto de datos implica un atributo de entrada y un valor para ese atributo. Se puede utilizar para dividir los patrones de entrenamiento en dos grupos de filas.\n",
    "\n",
    "La puntuación Gini da una idea de cuán buena es una división por cuán mezcladas están las clases en los dos grupos creados por la división. Una separación perfecta da como resultado una puntuación Gini de 0, mientras que la división en el peor de los casos que da como resultado 50/50 clases en cada grupo da como resultado una puntuación de Gini de 0,5 (para un problema de 2 clases)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "lNB5vQQGH1qQ"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jp3JCt8H1qS"
   },
   "source": [
    "## Regularización con hiperparámetros\n",
    "\n",
    "**Los árboles de decisión hacen muy pocas suposiciones sobre los datos de entrenamiento**.\n",
    "Si no se restringe, un árbol de decisión se adaptará para ajustarse perfectamente a\n",
    "los datos de entrenamiento, lo que naturalmente conduce al sobreajuste (iverfitting).\n",
    "\n",
    "Dicho modelo a menudo se denomina modelo no paramétrico porque la cantidad de parámetros\n",
    "no se determina antes del entrenamiento.\n",
    "\n",
    "Al menos podemos restringir la profundidad máxima del árbol de decisión, entre otros hiperparámetros de regularización:\n",
    "\n",
    "* min_samples_split: el número mínimo de muestras que debe tener un nodo para que se divida.\n",
    "* min_samples_leaf: El número mínimo de muestras que debe tener una hoja.\n",
    "* min_weight_fraction_leaf: mean_samples_leaf como una fracción.\n",
    "* max_leaf_nodes: el número máximo de nodos hoja.\n",
    "* max_features: el número máximo de características que se evalúan para cualquier división.\n",
    "\n",
    "La siguiente figura muestra dos árboles de decisión entrenados en el mismo conjunto de datos,\n",
    "el de la izquierda representa un árbol de decisión entrenado sin restricciones y el de\n",
    "la derecha se regulariza mediante el hiperparámetro min_samples_leaf:\n",
    "\n",
    "![RegularizedTree](img/regularized_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqT4CYheH1qS"
   },
   "source": [
    "## Ejemplo de Arboles de Decisión para Regresión\n",
    "\n",
    "Los árboles de decisión también son capaces de realizar tareas de regresión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XE_vDa-rH1qV"
   },
   "source": [
    "## Inestabilidad\n",
    "\n",
    "Los árboles de decisión tienen algunas limitaciones:\n",
    "\n",
    "* Los árboles de decisión prefieren los límites de decisión ortogonales, lo que los hace sensibles a la rotación del conjunto de entrenamiento. Una forma de limitar este problema es usar PCA (Análisis de Componentes Principales) que a menudo da como resultado una mejor orientación de los datos de entrenamiento.\n",
    "* Los árboles de decisión son sensibles a pequeñas variaciones en los datos de entrenamiento. De hecho, debido a que scikit-learn usa optimización estocástica, es posible que obtenga diferentes modelos para el mismo conjunto de datos de entrenamiento.\n",
    "\n",
    "Random Forests puede resolver este problema promediando la predicción entrante de muchos árboles de decisión. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RBExKyRaH1qR"
   },
   "source": [
    "## Complejidad computational\n",
    "\n",
    "Hacer una predicción requiere que vayamos desde la raíz hasta la última hoja.\n",
    "\n",
    "Los árboles de decisión están aproximadamente equilibrados, por lo que atravesar el árbol de decisión requiere recorrer aproximadamente $O(log_{2}(m))$. Dado que cada nodo requiere verificar el valor de una sola característica, el tiempo de ejecución de la inferencia general es $O(log_{2}(m))$. Lo que hace que el algoritmo sea independiente del número de características. Por lo tanto, las predicciones son realmente rápidas, incluso cuando el algoritmo se ocupa de una gran cantidad de características.\n",
    "\n",
    "El algoritmo de entrenamiento compara todas las características (excepto si se establece `max_features`) en todas las muestras en cada nodo, lo que da como resultado una complejidad de entrenamiento de $O(n \\times mlog_2(m))$.\n",
    "\n",
    "Para conjuntos de entrenamiento pequeños (menos de unos pocos miles), scikit-learn puede acelerar el entrenamiento mediante la clasificación previa de los datos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KguR0QJLH1qR"
   },
   "source": [
    "## EXTRA: The CART Training Algorithm\n",
    "\n",
    "Scikit-Learn utiliza el algoritmo del árbol de clasificación y regresión (CART) para entrenar árboles de decisión (también llamados árboles \"crecientes\"). El algoritmo funciona dividiendo primero el conjunto de entrenamiento por característica $k$ y umbral $t_k$.\n",
    "\n",
    "Elige $k$ y $t_k$ buscando los $(k,t_k)$ que producen los subconjuntos más puros ponderados por su tamaño.\n",
    "\n",
    "La siguiente figura da la función de pérdida que CART intenta minimizar:\n",
    "\n",
    "$$J(k,t_k)=\\frac{m_{left}}{m}G_{left} + \\frac{m_{right}}{m}G_{right}$$\n",
    "\n",
    "Donde:\n",
    "- $G_{left/right}$ mide la impureza resultante en los subconjuntos izquierdo/derecho.\n",
    "- $m_{izquierda/derecha}$ corresponde al número de instancias en los subconjuntos izquierdo/derecho.\n",
    "\n",
    "Una vez que el algoritmo CART dividió con éxito los datos de entrenamiento iniciales en dos subconjuntos, hace lo mismo con ambos subconjuntos. Deja de repetirse una vez que alcanza la profundidad de árbol máxima permitida (el hiperparámetro `max_depth`), o si no puede encontrar una división que reduzca la impureza.\n",
    "\n",
    "Otros hiperparámetros que controlan la detención incluyen: `min_samples_split`, `min_samples_leaf`, `min_weight_fraction_leaf`, `max_leaf_nodes`.\n",
    "\n",
    "El algoritmo CART es codicioso en el sentido de que no le importa si su división actual conducirá a una hoja descendente óptima. Solo se preocupa por encontrar la mejor división posible en la hoja actual. En ese sentido, no necesariamente resulta en una solución óptima.\n",
    "\n",
    "Desafortunadamente, se sabe que encontrar el árbol óptimo es un problema **NP-Completo** con una complejidad de $O(exp(m))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "yDZ9jT4gH1qS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "rG3nVHySH1qS"
   },
   "outputs": [],
   "source": [
    "# First we want to generate a noisy quadratic dataset\n",
    "X = np.linspace(start=0, stop=1, num=500)\n",
    "y = (X-0.5)**2 + np.random.randn(500)/50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "2X8oxvRxH1qT",
    "outputId": "0871d556-7cdb-4422-fba2-ec635ccb9140"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.collections.PathCollection at 0x23cd5b57580>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X, y, s=1.5, c='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "z8Krhab5H1qT"
   },
   "outputs": [],
   "source": [
    "tree_reg = DecisionTreeRegressor(max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "JjUf2Ir_H1qT",
    "outputId": "84d3ad43-61d4-4dd7-ebf0-36b22772435c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "DecisionTreeRegressor(max_depth=2)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_reg.fit(X[..., None], y[..., None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqddoAtTH1qT"
   },
   "source": [
    "Si comprobamos el árbol resultante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "y7PZlgdlH1qU"
   },
   "outputs": [],
   "source": [
    "export_graphviz(tree_reg, \n",
    "                out_file='./img/reg_tree.dot',\n",
    "                feature_names=['X'],\n",
    "                class_names=['y'],\n",
    "                rounded=True,\n",
    "                filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "KGDpCVn4H1qU",
    "outputId": "8ee87a5c-8f05-4dfc-ea62-01689c725f60"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: gvwrite_no_z problem 8192\n"
     ]
    }
   ],
   "source": [
    "! dot -Tpng ./img/reg_tree.dot -o ./img/reg_tree.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P62A8mHnH1qU"
   },
   "source": [
    "![RegressionTree](img/reg_tree.png)\n",
    "\n",
    "Este árbol se parece mucho al árbol de clasificación que creamos anteriormente. La principal diferencia es que en\n",
    "lugar de predecir una clase para cada nodo, predice un valor. La predicción representa el valor objetivo promedio\n",
    "para el grupo en la hoja.\n",
    "\n",
    "A medida que aumentamos el hiperparámetro `max_ depth`, proporcionamos más flexibilidad al árbol de regresión,\n",
    "a continuación se muestran las predicciones del árbol en rojo:\n",
    "\n",
    "![RegressionTrees](img/regression_trees.png)\n",
    "\n",
    "El algoritmo CART funciona casi igual que antes, pero en lugar de buscar una división que minimice la impureza,\n",
    "busca una división que produzca muestras equilibradas por hoja y minimice $MSE$.\n",
    "\n",
    "Mostramos la función de coste que el algoritmo intenta minimizar: \n",
    "\n",
    "$$J(k,t_k)=\\frac{m_{left}}{m}MSE_{left} + \\frac{m_{right}}{m}MSE_{right} \\\\ MSE=\\frac{1}{m}\\sum_{i=1}^{m}(\\hat{y}_{i}-y_{i})^{2}$$\n",
    "\n",
    "Al igual que la clasificación, los árboles de regresión son propensos a sobreajustar los datos de entrenamiento,\n",
    "sin ninguna regularización, terminamos con el gráfico de la izquierda y al configurar `min_samples_leaf=10` se\n",
    "produce un modelo mucho más razonable:\n",
    "\n",
    "![Regularizing](img/regularizing_trees.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXUn6BsDH1qd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "colab": {
   "name": "8.1.Supervised-DecisionTrees-RandomForest.ipynb",
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}